apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: parallel-training
spec:
  serviceAccountName: workflow
  volumes:
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: 64Gi
  entrypoint: main
  arguments:
    parameters:
      - name: training-conf-list
        value: '[
          {"embedding_dim":"80", "num_tokens": "10000", "revision":"NAF2008"},
          {"embedding_dim":"80", "num_tokens": "10000", "revision":"NAF2025"}
            ]'
  templates:
    - name: main
      # Entrypoint DAG template
      dag:
        tasks:
          - name: run-training-with-params
            template: run-training-wt
            arguments:
              parameters:
                - name: embedding_dim
                  value: "{{item.embedding_dim}}"
                - name: num_tokens
                  value: "{{item.num_tokens}}"
                - name: revision
                  value: "{{item.revision}}"

            # Pass the inputs to the task using "withParam"
            withParam: "{{workflow.parameters.training-conf-list}}"

    # Worker template for task-1
    - name: run-training-wt
      inputs:
        parameters:
          - name: embedding_dim
          - name: num_tokens
          - name: revision
      nodeSelector:
        nvidia.com/gpu.product: "NVIDIA-H100-PCIe"
      container:
        image: inseefrlab/onyxia-vscode-pytorch:py3.13.5-gpu
        imagePullPolicy: Always
        resources:
          limits:
            nvidia.com/gpu: 1
            cpu: 30000m
            memory: 50Gi
          requests:
              cpu: 100m
              memory: 2Gi
        command: ["/bin/bash", -c]
        args:
          - |
            set -e  # Exit on error
            set -x  # Print commands for debugging

            # Clone repository and submodules
            git clone -b main https://github.com/InseeFrLab/codif-ape-train.git
            cd codif-ape-train/

            # Setup
            . setup.sh

            # Run MLflow experiment
            uv run src/train.py -m model.model_params.embedding_dim={{inputs.parameters.embedding_dim}} tokenizer.num_tokens={{inputs.parameters.num_tokens}} revision={{inputs.parameters.revision}}

        volumeMounts:
          - mountPath: /dev/shm
            name: dshm
        env:
          # env var for s3 connexion
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: my-s3-creds
                key: accessKey
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: my-s3-creds
                key: secretKey
          - name: AWS_DEFAULT_REGION
            value: us-east-1
          - name: AWS_S3_ENDPOINT
            value: minio.lab.sspcloud.fr
          - name: MLFLOW_S3_ENDPOINT_URL
            value: https://minio.lab.sspcloud.fr
          - name: MLFLOW_TRACKING_URI
            value: https://projet-ape-mlflow.user.lab.sspcloud.fr/
