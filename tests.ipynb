{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"src\"))\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import hydra\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from joblib import Memory\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "from evaluators import torchFastTextEvaluator\n",
    "from framework_classes import (\n",
    "    DATA_GETTER,\n",
    "    DATASETS,\n",
    "    LOSSES,\n",
    "    MODELS,\n",
    "    MODULES,\n",
    "    OPTIMIZERS,\n",
    "    PREPROCESSORS,\n",
    "    SCHEDULERS,\n",
    "    TOKENIZERS,\n",
    "    TRAINERS,\n",
    ")\n",
    "from models import FastTextWrapper\n",
    "from utils.data import get_df_naf, get_file_system, get_processed_data, get_test_data, get_Y\n",
    "from utils.mappings import mappings\n",
    "from utils.mlflow import create_or_restore_experiment\n",
    "from utils.validation_viz import calibration_curve, confidence_histogram, sort_and_get_pred\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revision = \"NAF2008\"\n",
    "model_class = \"torchFastText\"\n",
    "start_month = 1\n",
    "start_year = 2018\n",
    "text_feature = \"libelle\"\n",
    "textual_features_1 = \"NAT_LIB\"\n",
    "textual_features_2 = \"AGRI\"\n",
    "categorical_features_1 = \"TYP\"\n",
    "categorical_features_2 = \"NAT\"\n",
    "categorical_features_3 = \"SRF\"\n",
    "categorical_features_4 = \"CJ\"\n",
    "categorical_features_5 = \"CRT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_dict = {\"data\": \n",
    "                {\"sirene\":\"sirene_4\", \n",
    "                \"start_month\": start_month, \n",
    "                \"start_year\": start_year, \n",
    "                \"revision\": revision,\n",
    "                \"text_feature\": text_feature,\n",
    "                \"textual_features\" : [textual_features_1, textual_features_2],\n",
    "                \"categorical_features\" : [categorical_features_1, categorical_features_2, categorical_features_3, categorical_features_4, categorical_features_5],}, \n",
    "                \n",
    "            \"model\":{\"name\": \"fastText\",\n",
    "                    \"preprocessor\": \"fastText\",\n",
    "                    \"test_params\": {\"test_batch_size\": 256, \"run_id\":'runs:/45afc22a961a4cdcb282aad93693326d/model'}}\n",
    "            }\n",
    "cfg_dict_data = cfg_dict[\"data\"]\n",
    "df_naf = get_df_naf(revision=cfg_dict_data[\"revision\"])\n",
    "Y = get_Y(revision=revision)\n",
    "df_test_ls= get_test_data(**cfg_dict[\"data\"], y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = get_Y(revision=cfg_dict[\"data\"][\"revision\"])\n",
    "df_train, df_val, df_test = get_processed_data(revision=cfg_dict[\"data\"][\"revision\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "logged_model = 'runs:/65bc7a269ea145248476b8c976090784/default'\n",
    "mlflow.set_tracking_uri(\"https://projet-ape-mlflow.user.lab.sspcloud.fr/\")\n",
    "# Load model as a PyFuncModel.\n",
    "fasttext = mlflow.pyfunc.load_model(logged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_preds = fasttext.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = cfg_dict[\"model\"][\"test_params\"][\"run_id\"]\n",
    "module = mlflow.pytorch.load_model(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_preds = torchFastTextEvaluator(module).get_preds(\n",
    "            df=df_test,\n",
    "            Y=Y,\n",
    "            **cfg_dict[\"data\"],\n",
    "            batch_size=cfg_dict[\"model\"][\"test_params\"][\"test_batch_size\"],\n",
    "            num_workers=os.cpu_count() - 1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = torchFastTextEvaluator(module).get_aggregated_preds(df=df_test, predictions=torch_preds, Y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = df_res[\"APE_NIV5\"]\n",
    "torchft_preds = df_res[\"APE_NIV5_pred_k1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_preds_labels = fasttext_preds[0]\n",
    "fasttext_preds_labels = [label[0][-5:] for label in fasttext_preds_labels]\n",
    "fasttext_preds_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torchft_preds == fasttext_preds_labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torchft_preds == ground_truth).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ground_truth == fasttext_preds_labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchft_niv4 = df_res[\"APE_NIV4_pred_k1\"]\n",
    "fasttext_preds_labels_niv4 = pd.Series(fasttext_preds_labels).str[:4]\n",
    "ground_truth_niv4 = df_res[\"APE_NIV4\"]\n",
    "print((torchft_niv4 == fasttext_preds_labels_niv4).mean())\n",
    "print((ground_truth_niv4 == fasttext_preds_labels_niv4).mean())\n",
    "print((ground_truth_niv4 == torchft_niv4).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_disagreements = ground_truth[torchft_preds != fasttext_preds_labels]\n",
    "torchft_disagreements = torchft_preds[torchft_preds != fasttext_preds_labels]\n",
    "fasttext_disagreements = pd.Series(fasttext_preds_labels)[torchft_preds != fasttext_preds_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreements = pd.DataFrame({\"ground_truth\": ground_truth_disagreements, \"torchft\": torchft_disagreements, \"fasttext\": fasttext_disagreements})\n",
    "disagreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some results\n",
    "sorted_confidence, well_predicted, predicted_confidence, predicted_class, true_values = (\n",
    "    sort_and_get_pred(predictions=torch_preds, df=df_test, Y=Y)\n",
    ")\n",
    "fig1 = confidence_histogram(sorted_confidence, well_predicted, df=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_preds_scores = np.array(fasttext_preds[1])\n",
    "fasttext_preds_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "well_predicted = (ground_truth == fasttext_preds_labels)\n",
    "print(well_predicted.shape)\n",
    "df = pd.DataFrame(\n",
    "        {\n",
    "            \"confidence_score\": fasttext_preds_scores.reshape((-1, )),\n",
    "            \"well_predicted\": well_predicted,  # Ensure this is categorical if needed\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Plot with proper data format\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.histplot(data=df, x=\"confidence_score\", bins=100, hue=\"well_predicted\", stat=\"percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot (accuracy, codif auto) pour diff√©rents niveau de seuil"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
