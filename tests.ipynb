{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('src/')\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import hydra\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from joblib import Memory\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "from evaluators import torchFastTextEvaluator\n",
    "from framework_classes import (\n",
    "    DATA_GETTER,\n",
    "    DATASETS,\n",
    "    LOSSES,\n",
    "    MODELS,\n",
    "    MODULES,\n",
    "    OPTIMIZERS,\n",
    "    PREPROCESSORS,\n",
    "    SCHEDULERS,\n",
    "    TOKENIZERS,\n",
    "    TRAINERS,\n",
    ")\n",
    "from utils.data import get_df_naf, get_file_system, get_processed_data, get_test_data, get_Y\n",
    "from utils.mappings import mappings\n",
    "from utils.mlflow import create_or_restore_experiment\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revision = \"NAF2008\"\n",
    "model_class = \"torchFastText\"\n",
    "start_month = 1\n",
    "start_year = 2018\n",
    "text_feature = \"libelle\"\n",
    "textual_features_1 = \"NAT_LIB\"\n",
    "textual_features_2 = \"AGRI\"\n",
    "categorical_features_1 = \"TYP\"\n",
    "categorical_features_2 = \"NAT\"\n",
    "categorical_features_3 = \"SRF\"\n",
    "categorical_features_4 = \"CJ\"\n",
    "categorical_features_5 = \"CRT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_dict = {\"data\": \n",
    "                {\"sirene\":\"sirene_4\", \n",
    "                \"start_month\": start_month, \n",
    "                \"start_year\": start_year, \n",
    "                \"revision\": revision,\n",
    "                \"text_feature\": text_feature,\n",
    "                \"textual_features\" : [textual_features_1, textual_features_2],\n",
    "                \"categorical_features\" : [categorical_features_1, categorical_features_2, categorical_features_3, categorical_features_4, categorical_features_5],}, \n",
    "                \n",
    "            \"model\":{\"name\": \"torchFastText\",\n",
    "                    \"preprocessor\": \"PyTorch\",}}\n",
    "cfg_dict_data = cfg_dict[\"data\"]\n",
    "df_naf = get_df_naf(revision=cfg_dict_data[\"revision\"])\n",
    "Y = get_Y(revision=revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PREPROCESSORS[cfg_dict[\"model\"][\"preprocessor\"]](cfg_dict)\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test_ls= get_test_data(**cfg_dict[\"data\"], y=Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test_ls = pd.concat(preprocessor.preprocess(df_test_ls,\n",
    "            df_naf=df_naf,\n",
    "            y=Y,\n",
    "            text_feature=cfg_dict_data[\"text_feature\"],\n",
    "            textual_features=cfg_dict_data[\"textual_features\"],\n",
    "            categorical_features=cfg_dict_data[\"categorical_features\"],\n",
    "            test_size=0.1,), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, df_test = get_processed_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"https://projet-ape-mlflow.user.lab.sspcloud.fr/\")\n",
    "mlflow.set_experiment('model_comparison_s4')\n",
    "logged_model = 'runs:/45afc22a961a4cdcb282aad93693326d/model'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "module = mlflow.pytorch.load_model(logged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = torchFastTextEvaluator(module)\n",
    "test_res = evaluator.launch_test(\n",
    "    df_test_ls,\n",
    "    text_feature=cfg_dict[\"data\"][\"text_feature\"],\n",
    "    categorical_features=cfg_dict[\"data\"][\"categorical_features\"],\n",
    "    Y=Y,\n",
    "    batch_size=256,\n",
    "    num_workers=72,\n",
    ")\n",
    "\n",
    "test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = evaluator.get_aggregated_preds(df=df_test_ls, Y=Y, **cfg_dict[\"data\"], batch_size=256, num_workers=72)\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay, roc_auc_score\n",
    "\n",
    "auc = roc_auc_score(df_test_ls[Y].values, predictions.detach().numpy(), multi_class='ovr', average=None)\n",
    "auc.argmin(), auc[auc.argmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "predictions = evaluator.get_preds(\n",
    "    df_test_ls,\n",
    "    **cfg_dict[\"data\"],\n",
    "    Y=Y,\n",
    "    batch_size=256,\n",
    "    num_workers=72,\n",
    ")\n",
    "\n",
    "sorted_confidence = predictions.sort(descending=True).values\n",
    "confidence_score = sorted_confidence[:, 0]\n",
    "\n",
    "well_predicted = (predictions.argmax(dim=1) == df_test_ls[Y].values).float()\n",
    "print(well_predicted.mean())\n",
    "# Convert NumPy arrays to a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"confidence_score\": confidence_score.numpy(),\n",
    "    \"well_predicted\": well_predicted.numpy()  # Ensure this is categorical if needed\n",
    "})\n",
    "\n",
    "# Plot with proper data format\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x=\"confidence_score\", bins=100, hue=\"well_predicted\", stat=\"percent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "predictions = evaluator.get_preds(\n",
    "    df_test,\n",
    "    **cfg_dict[\"data\"],\n",
    "    Y=Y,\n",
    "    batch_size=256,\n",
    "    num_workers=72,\n",
    ")\n",
    "\n",
    "# Number of bins for calibration\n",
    "n_bins = 100  \n",
    "\n",
    "# Convert logits to probabilities\n",
    "num_classes = 732  # Adjust based on your setup\n",
    "\n",
    "# Get the highest predicted probability and corresponding class\n",
    "confidences, predicted_classes = torch.max(predictions, dim=1)  # Max probability per sample\n",
    "true_labels = torch.tensor(df_test[Y].values)  # True labels\n",
    "\n",
    "# Initialize bins\n",
    "bin_boundaries = torch.linspace(0, 1, n_bins + 1)  # Bins from 0 to 1\n",
    "bin_accs = []\n",
    "bin_confidences = []\n",
    "\n",
    "# Compute accuracy per bin\n",
    "for i in range(n_bins):\n",
    "    bin_mask = (confidences >= bin_boundaries[i]) & (confidences < bin_boundaries[i + 1])\n",
    "    if bin_mask.sum() > 0:\n",
    "        bin_acc = (predicted_classes[bin_mask] == true_labels[bin_mask]).float().mean()\n",
    "        bin_conf = confidences[bin_mask].mean()\n",
    "        bin_accs.append(bin_acc.item())\n",
    "        bin_confidences.append(bin_conf.item())\n",
    "\n",
    "# Convert to numpy\n",
    "bin_confidences = np.array(bin_confidences)\n",
    "bin_accs = np.array(bin_accs)\n",
    "\n",
    "# Plot reliability diagram\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"black\", label=\"Perfect Calibration\")\n",
    "plt.plot(bin_confidences, bin_accs, marker=\"o\", color=\"blue\", label=\"Model Calibration\")\n",
    "plt.xlabel(\"Confidence\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Reliability Diagram\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
