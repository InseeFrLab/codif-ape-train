{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"src\"))\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import hydra\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from joblib import Memory\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "from evaluators import torchFastTextEvaluator\n",
    "from framework_classes import (\n",
    "    DATA_GETTER,\n",
    "    DATASETS,\n",
    "    LOSSES,\n",
    "    MODELS,\n",
    "    MODULES,\n",
    "    OPTIMIZERS,\n",
    "    PREPROCESSORS,\n",
    "    SCHEDULERS,\n",
    "    TOKENIZERS,\n",
    "    TRAINERS,\n",
    ")\n",
    "from models import FastTextWrapper\n",
    "from utils.data import get_df_naf, get_file_system, get_processed_data, get_test_data, get_Y\n",
    "from utils.mappings import mappings\n",
    "from utils.mlflow import create_or_restore_experiment\n",
    "from utils.validation_viz import calibration_curve, confidence_histogram, sort_and_get_pred\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revision = \"NAF2008\"\n",
    "model_class = \"torchFastText\"\n",
    "start_month = 1\n",
    "start_year = 2018\n",
    "text_feature = \"libelle\"\n",
    "textual_features_1 = \"NAT_LIB\"\n",
    "textual_features_2 = \"AGRI\"\n",
    "categorical_features_1 = \"TYP\"\n",
    "categorical_features_2 = \"NAT\"\n",
    "categorical_features_3 = \"SRF\"\n",
    "categorical_features_4 = \"CJ\"\n",
    "categorical_features_5 = \"CRT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_dict = {\"data\": \n",
    "                {\"sirene\":\"sirene_4\", \n",
    "                \"start_month\": start_month, \n",
    "                \"start_year\": start_year, \n",
    "                \"revision\": revision,\n",
    "                \"text_feature\": text_feature,\n",
    "                \"textual_features\" : [textual_features_1, textual_features_2],\n",
    "                \"categorical_features\" : [categorical_features_1, categorical_features_2, categorical_features_3, categorical_features_4, categorical_features_5],}, \n",
    "                \n",
    "            \"model\":{\"name\": \"fastText\",\n",
    "                    \"preprocessor\": \"fastText\",\n",
    "                    \"test_params\": {\"test_batch_size\": 256, \"run_id\":'runs:/45afc22a961a4cdcb282aad93693326d/model'}}\n",
    "            }\n",
    "cfg_dict_data = cfg_dict[\"data\"]\n",
    "df_naf = get_df_naf(revision=cfg_dict_data[\"revision\"])\n",
    "Y = get_Y(revision=revision)\n",
    "df_test_ls= get_test_data(**cfg_dict[\"data\"], y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = get_Y(revision=cfg_dict[\"data\"][\"revision\"])\n",
    "df_train, df_val, df_test = get_processed_data(revision=cfg_dict[\"data\"][\"revision\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(mlflow.pyfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import mlflow\n",
    "\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = 'https://minio.lab.sspcloud.fr'\n",
    "# Load model as a PyFuncModel.\n",
    "logged_model = 'runs:/d22c02c2df384c549c315b49af338988/default'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_preds = fasttext.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"https://projet-ape-mlflow.user.lab.sspcloud.fr/\")\n",
    "run_id = cfg_dict[\"model\"][\"test_params\"][\"run_id\"]\n",
    "module = mlflow.pytorch.load_model(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_preds = torchFastTextEvaluator(module).get_preds(\n",
    "            df=df_test,\n",
    "            Y=Y,\n",
    "            **cfg_dict[\"data\"],\n",
    "            batch_size=cfg_dict[\"model\"][\"test_params\"][\"test_batch_size\"],\n",
    "            num_workers=os.cpu_count() - 1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = torchFastTextEvaluator(module).get_aggregated_preds(df=df_test, predictions=torch_preds, Y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = df_res[\"APE_NIV5\"]\n",
    "torchft_preds = df_res[\"APE_NIV5_pred_k1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_preds_labels = fasttext_preds[0]\n",
    "fasttext_preds_labels = [label[0][-5:] for label in fasttext_preds_labels]\n",
    "fasttext_preds_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((torchft_preds == fasttext_preds_labels).mean())\n",
    "print((torchft_preds == ground_truth).mean())\n",
    "print((ground_truth == fasttext_preds_labels).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some results\n",
    "sorted_confidence, well_predicted, predicted_confidence, predicted_class, true_values = (\n",
    "    sort_and_get_pred(predictions=torch_preds, df=df_test, Y=Y)\n",
    ")\n",
    "fig1 = confidence_histogram(sorted_confidence, well_predicted, df=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_automatic_accuracy(thresholds, predicted_confidence, predicted_class, true_values):\n",
    "\n",
    "    \"\"\"\n",
    "    We automatically code the APE if the confidence is above the threshold.\n",
    "    Compute the accuracy on the automatically coded APEs.\n",
    "\n",
    "    Args:\n",
    "        thresholds (np.array(float), shape (n_thresholds,)): The threshold for automatic coding.\n",
    "        predicted_confidence (torch.Tensor, shape (n_samples, 1): The confidence of the predictions.\n",
    "        predicted_class (torch.Tensor, shape (n_samples, 1)\n",
    "            The predicted class of the APEs.\n",
    "        true_values (torch.Tensor, shape (n_samples, 1): The true values of the APEs.\n",
    "\n",
    "    Returns:\n",
    "        automatic_coding_rate (float): The rate of automatically coded APEs.\n",
    "        accuracy_automatic (float): The accuracy on the automatically coded APEs.\n",
    "\n",
    "    \"\"\"\n",
    "    n_thresholds = len(thresholds)\n",
    "    automatic_coding_mask = predicted_confidence[:, None] > thresholds[None, :]\n",
    "    automatic_coding_rate = automatic_coding_mask.mean(axis=0)\n",
    "\n",
    "    predicted_class_expanded = np.repeat(predicted_class[:, None], n_thresholds, 1)\n",
    "    true_values_expanded = np.repeat(true_values[:, None], n_thresholds, 1)\n",
    "    \n",
    "    predicted_automatic = np.ma.array(predicted_class_expanded, mask=~automatic_coding_mask)\n",
    "    ground_truth_automatic = np.ma.array(true_values_expanded, mask=~automatic_coding_mask)\n",
    "    accuracy_automatic = (predicted_automatic == ground_truth_automatic).mean(axis=0)\n",
    "    return automatic_coding_rate, accuracy_automatic\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 100)\n",
    "torchft_plot =  get_automatic_accuracy(thresholds, predicted_confidence.numpy(), predicted_class.numpy(), true_values)\n",
    "fasttext_preds_scores = np.array(fasttext_preds[1])\n",
    "fasttext_preds_labels = np.array(fasttext_preds_labels)\n",
    "ft_plot =  get_automatic_accuracy(thresholds, np.clip(fasttext_preds_scores.reshape(-1), 0, 1), fasttext_preds_labels.reshape(-1), ground_truth.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file.py\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming thresholds, torchft_plot, fasttext_preds_scores, fasttext_preds_labels, and ground_truth are already defined\n",
    "\n",
    "# Create masks for the plots\n",
    "mask_torchft = torchft_plot[0] > 0\n",
    "mask_ft = ft_plot[0] > 0\n",
    "\n",
    "# Create the Plotly figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for torchft\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=torchft_plot[0][mask_torchft],\n",
    "    y=torchft_plot[1][mask_torchft],\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    text=[f'Threshold: {thresh}' for thresh in thresholds[mask_torchft]],\n",
    "    name='torchft'\n",
    "))\n",
    "\n",
    "# Add traces for ft\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=ft_plot[0][mask_ft],\n",
    "    y=ft_plot[1][mask_ft],\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    text=[f'Threshold: {thresh}' for thresh in thresholds[mask_ft]],\n",
    "    name='ft'\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Pourcentage de codif automatique\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    legend=dict(\n",
    "        x=1,\n",
    "        y=1,\n",
    "        traceorder=\"normal\",\n",
    "        font=dict(\n",
    "            family=\"sans-serif\",\n",
    "            size=12,\n",
    "            color=\"black\"\n",
    "        ),\n",
    "        bgcolor=\"LightSteelBlue\",\n",
    "        bordercolor=\"Black\",\n",
    "        borderwidth=2\n",
    "    ),\n",
    "    width=800,  # Set the figure width\n",
    "    height=600   # Set the figure height\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "well_predicted = (ground_truth == fasttext_preds_labels)\n",
    "print(well_predicted.shape)\n",
    "df = pd.DataFrame(\n",
    "        {\n",
    "            \"confidence_score\": fasttext_preds_scores.reshape((-1, )),\n",
    "            \"well_predicted\": well_predicted,  # Ensure this is categorical if needed\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Plot with proper data format\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.histplot(data=df, x=\"confidence_score\", bins=100, hue=\"well_predicted\", stat=\"percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
