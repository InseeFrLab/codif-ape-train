name: "torchFastText"
preprocessor: "PyTorch"
dataset: "FastTextModelDataset"
model_params:
  embedding_dim: 180
  categorical_embedding_dims: 5
  sparse: False
  direct_bagging: True

training_params:
  trainer_name: "Lightning"
  num_epochs: 10
  patience_early_stopping: 3
  batch_size: 256
  optimizer_name: "Adam"
  optimizer_params:
    lr: 1e-4
  scheduler_name: "ReduceLROnPlateau"
  scheduler_params:
    factor: 0.5
    patience: 2
    min_lr: 1e-6
  loss_name: "CrossEntropyLoss"
