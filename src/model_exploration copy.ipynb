{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import fasttext\n",
    "\n",
    "from constants import TEXT_FEATURE\n",
    "from fasttext_classifier.fasttext_evaluator import FastTextEvaluator\n",
    "from fasttext_classifier.fasttext_preprocessor import FastTextPreprocessor\n",
    "from fasttext_classifier.fasttext_trainer import FastTextTrainer\n",
    "from fasttext_classifier.fasttext_wrapper import FastTextWrapper\n",
    "from utils import get_root_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Preprocessing the database...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_path = \"config/config_fasttext41.yaml\"\n",
    "\n",
    "preprocessor = FastTextPreprocessor()\n",
    "trainer = FastTextTrainer()\n",
    "\n",
    "print(\"*** Preprocessing the database...\\n\")\n",
    "# Load data, assumed to be stored in a .parquet file\n",
    "df = pd.read_parquet(\"../data/extraction_sirene_20220712_harmonised.parquet\", engine=\"pyarrow\")\n",
    "df = df.sample(frac=0.001)\n",
    "\n",
    "with open(get_root_path() / config_path, \"r\") as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "params = config[\"params\"]\n",
    "categorical_features = config[\"categorical_features\"]\n",
    "Y = config[\"Y\"][0]\n",
    "oversampling = config[\"oversampling\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"APE_SICORE\": \"APE_NIV5\"})\n",
    "try:\n",
    "    df_naf = pd.read_csv(r\"./data/naf_extended.csv\", dtype=str)\n",
    "except FileNotFoundError:\n",
    "    df_naf = pd.read_csv(r\"../data/naf_extended.csv\", dtype=str)\n",
    "df_naf[[\"NIV3\", \"NIV4\", \"NIV5\"]] = df_naf[[\"NIV3\", \"NIV4\", \"NIV5\"]].apply(\n",
    "    lambda x: x.str.replace(\".\", \"\", regex=False)\n",
    ")\n",
    "\n",
    "df_naf = df_naf.rename(columns={f\"NIV{i}\" : f\"APE_NIV{i}\" for i in range(1,6)})\n",
    "df_naf = df_naf[[f\"APE_NIV{i}\" for i in range(1,6)] + [\"LIB_NIV5\"]]\n",
    "df = df.join(df_naf.set_index(\"APE_NIV5\"), on=\"APE_NIV5\")\n",
    "MissingCodes = set(df_naf[\"APE_NIV5\"]) - set(df[\"APE_NIV5\"])\n",
    "\n",
    "Fake_obs = df_naf[df_naf.APE_NIV5.isin(MissingCodes)]\n",
    "Fake_obs.loc[:,\"LIB_SICORE\"] = Fake_obs.LIB_NIV5\n",
    "Fake_obs.loc[:,\"DATE\"] = pd.Timestamp.today()\n",
    "df = pd.concat([df, Fake_obs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General preprocessing\n",
    "variables = [Y] + [TEXT_FEATURE]\n",
    "if categorical_features is not None:\n",
    "    variables += categorical_features\n",
    "    df[categorical_features] = df[categorical_features].fillna(value=\"NaN\")\n",
    "df = df[\n",
    "    variables\n",
    "    + [\"APE_NIV\" + str(i) for i in range(1, 6) if str(i) not in [Y[-1]]]\n",
    "]\n",
    "df = df.dropna(subset=[Y] + [TEXT_FEATURE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "df_train, df_test, df_gu = preprocessor.preprocess(\n",
    "    df=df,\n",
    "    y=Y,\n",
    "    text_feature=TEXT_FEATURE,\n",
    "    categorical_features=categorical_features,\n",
    "    oversampling=oversampling,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 1M words\n",
      "Number of words:  4722\n",
      "Number of labels: 245\n",
      "Progress: 100.0% words/sec/thread:   30052 lr:  0.000000 avg.loss:  0.877309 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = trainer.train(df_train, Y, TEXT_FEATURE, categorical_features, params)\n",
    "model.save_model(\"../data/temp3.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "#model = trainer.train(df_train, Y, TEXT_FEATURE, categorical_features, params)\n",
    "#model.save_model(\"../data/temp.bin\")\n",
    "model = fasttext.load_model(\"../data/temp.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Evaluating the model...\n",
      "\n",
      "Evaluation lasted 2.1 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"*** Evaluating the model...\\n\")\n",
    "t = time.time()\n",
    "\n",
    "evaluator = FastTextEvaluator(model)\n",
    "accuracies = evaluator.evaluate(\n",
    "df_test, Y, TEXT_FEATURE, categorical_features, 2\n",
    ")\n",
    "print(f\"Evaluation lasted {round(time.time() - t,1)} seconds.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Evaluating the model...\n",
      "\n",
      "Evaluation lasted 1.6 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"*** Evaluating the model...\\n\")\n",
    "t = time.time()\n",
    "\n",
    "evaluator = FastTextEvaluator(model)\n",
    "accuracies = compute_accuracies(get_aggregated_preds(df_test, Y, TEXT_FEATURE, categorical_features, 2), Y)\n",
    "print(f\"Evaluation lasted {round(time.time() - t,1)} seconds.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(\n",
    "    df,\n",
    "    y,\n",
    "    text_feature,\n",
    "    categorical_features,\n",
    "    k):\n",
    "    \"\"\"\n",
    "    Returns the prediction of the model for pd.DataFrame `df`\n",
    "    along with the output probabilities.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Evaluation DataFrame.\n",
    "        y (str): Name of the variable to predict.\n",
    "        text_feature (str): Name of the text feature.\n",
    "        categorical_features (Optional[List[str]]): Names of the\n",
    "            categorical features.\n",
    "        k (int): Number of predictions.\n",
    "\n",
    "    Returns:\n",
    "        List: List with the prediction and probability for the\n",
    "            given text.\n",
    "    \"\"\"\n",
    "    libs = []\n",
    "\n",
    "    iterables_features = (\n",
    "        categorical_features if categorical_features is not None else []\n",
    "    )\n",
    "    for item in df.iterrows():\n",
    "        formatted_item = item[1][text_feature]\n",
    "        for feature in iterables_features:\n",
    "            formatted_item += f\" {feature}_{item[1][feature]}\"\n",
    "        libs.append(formatted_item)\n",
    "\n",
    "    res = model.predict(libs, k=k)\n",
    "    return {\n",
    "        rank_pred: [\n",
    "            (x[rank_pred].replace(\"__label__\", \"\"), y[rank_pred])\n",
    "            for x, y in zip(res[0], res[1])\n",
    "        ]\n",
    "        for rank_pred in range(k)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregated_preds(df, y, text_feature, categorical_features, k):\n",
    "\n",
    "    preds = get_preds(df, y, text_feature, categorical_features, k)\n",
    "    level = int(y[-1])\n",
    "\n",
    "    predicted_classes = {\n",
    "        f\"predictions_{level}_k{rank_pred+1}\": [pred[0] for pred in preds[rank_pred]] for rank_pred in range(k)\n",
    "    }\n",
    "    probs_prediction = {\n",
    "        f\"probabilities_k{rank_pred+1}\": [prob[1] for prob in preds[rank_pred]] for rank_pred in range(k)\n",
    "    }\n",
    "    liasseNb = df.index\n",
    "\n",
    "    preds_df = pd.DataFrame(predicted_classes)\n",
    "    preds_df.set_index(liasseNb, inplace= True)\n",
    "    \n",
    "    proba_df = pd.DataFrame(probs_prediction)\n",
    "    proba_df.set_index(liasseNb, inplace= True)\n",
    "\n",
    "    try:\n",
    "        df_naf = pd.read_csv(r\"./data/naf_extended.csv\", dtype=str)\n",
    "    except FileNotFoundError:\n",
    "        df_naf = pd.read_csv(r\"../data/naf_extended.csv\", dtype=str)\n",
    "        \n",
    "    df_naf[[\"NIV3\", \"NIV4\", \"NIV5\"]] = df_naf[[\"NIV3\", \"NIV4\", \"NIV5\"]].apply(\n",
    "        lambda x: x.str.replace(\".\", \"\", regex=False)\n",
    "    )\n",
    "    df_naf = df_naf[[f\"NIV{i}\"for i in range(1, level+1)]]\n",
    "\n",
    "    for rank_pred in range(k):\n",
    "        df_naf_renamed = df_naf.rename(columns= {f\"NIV{i}\" : f\"predictions_{i}_k{rank_pred+1}\" for i in range(1, level+1)})\n",
    "        preds_df = preds_df.join(df_naf_renamed.set_index(f\"predictions_{level}_k{rank_pred+1}\"), on=f\"predictions_{level}_k{rank_pred+1}\")\n",
    "        preds_df = preds_df[~preds_df.index.duplicated(keep='first')]\n",
    "\n",
    "    df = df.rename(columns= {f\"APE_NIV{i}\" : f\"ground_truth_{i}\" for i in range(1, level+1)}) \n",
    "\n",
    "    return df.join(preds_df.join(proba_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracies(df, y):\n",
    "    \"\"\"\n",
    "    Computes accuracies (for different levels of the NAF classification)\n",
    "    of the trained model on DataFrame `df`.\n",
    "\n",
    "    Args:\n",
    "        aggregated_APE_dict (Dict[int, pd.DataFrame]): Dictionary\n",
    "            of true and predicted labels at each level of the NAF\n",
    "            classification.\n",
    "        k (int): Number of predictions.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: Accuracies dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    level = int(y[-1])\n",
    "    accuracies = {\n",
    "        f\"accuracy_level_{aLevel}\": np.mean(\n",
    "            (\n",
    "                df[f\"predictions_{aLevel}_k1\"]\n",
    "                == df[f\"ground_truth_{aLevel}\"]\n",
    "            )\n",
    "        )\n",
    "        for aLevel in range(1, level+1)\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        accuracies\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed5946f2a0d3f5e934efd92075c2a89b2cb5130d0efd6e4509a568bedf48ed26"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('basesspcloud')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
