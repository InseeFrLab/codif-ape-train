{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hydra'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhydra\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlflow\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'hydra'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"src\"))\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import hydra\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from joblib import Memory\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "from evaluators import Evaluator, torchFastTextEvaluator\n",
    "from framework_classes import (\n",
    "        DATA_GETTER,\n",
    "        DATASETS,\n",
    "        LOSSES,\n",
    "        MODELS,\n",
    "        MODULES,\n",
    "        OPTIMIZERS,\n",
    "        PREPROCESSORS,\n",
    "        SCHEDULERS,\n",
    "        TOKENIZERS,\n",
    "        TRAINERS,\n",
    ")\n",
    "from models import FastTextWrapper\n",
    "from utils.data import PATHS, get_df_naf, get_file_system, get_processed_data, get_test_data, get_Y\n",
    "from utils.mappings import mappings\n",
    "from utils.mlflow import create_or_restore_experiment\n",
    "from utils.validation_viz import (\n",
    "        calibration_curve,\n",
    "        confidence_histogram,\n",
    "        get_automatic_accuracy,\n",
    "        sort_and_get_pred,\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revision = \"NAF2008\"\n",
    "model_class = \"torchFastText\"\n",
    "start_month = 1\n",
    "start_year = 2018\n",
    "text_feature = \"libelle\"\n",
    "textual_features_1 = \"NAT_LIB\"\n",
    "textual_features_2 = \"AGRI\"\n",
    "categorical_features_1 = \"TYP\"\n",
    "categorical_features_2 = \"NAT\"\n",
    "categorical_features_3 = \"SRF\"\n",
    "categorical_features_4 = \"CJ\"\n",
    "categorical_features_5 = \"CRT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_dict = {\"data\": \n",
    "                {\"sirene\":\"sirene_4\", \n",
    "                \"start_month\": start_month, \n",
    "                \"start_year\": start_year, \n",
    "                \"revision\": revision,\n",
    "                \"text_feature\": text_feature,\n",
    "                \"textual_features\" : [textual_features_1, textual_features_2],\n",
    "                \"categorical_features\" : [categorical_features_1, categorical_features_2, categorical_features_3, categorical_features_4, categorical_features_5],}, \n",
    "                \n",
    "            \"model\":{\"name\": \"PyTorch\",\n",
    "                    \"preprocessor\": \"PyTorch\",\n",
    "                    \"test_params\": {\"test_batch_size\": 256, \"run_id\":'runs:/45afc22a961a4cdcb282aad93693326d/model'}}\n",
    "            }\n",
    "cfg_dict_data = cfg_dict[\"data\"]\n",
    "df_naf = get_df_naf(revision=cfg_dict_data[\"revision\"])\n",
    "Y = get_Y(revision=cfg_dict[\"data\"][\"revision\"])\n",
    "df_test_ls= get_test_data(**cfg_dict[\"data\"], y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s3, df_s4 = DATA_GETTER[cfg_dict_data[\"sirene\"]](**cfg_dict[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = get_processed_data(revision=cfg_dict[\"data\"][\"revision\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_preds = torchFastTextEvaluator(module).get_preds(\n",
    "        df=df_test,\n",
    "        Y=Y,\n",
    "        **cfg_dict[\"data\"],\n",
    "        batch_size=cfg_dict[\"model\"][\"test_params\"][\"test_batch_size\"],\n",
    "        num_workers=os.cpu_count() - 1,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = get_file_system()\n",
    "df_res = pd.read_parquet( PATHS[cfg_dict[\"data\"][\"revision\"]][-1][:-8] + f\"_predictions_torch.parquet\", filesystem=fs)\n",
    "df_res_ft = pd.read_parquet( PATHS[cfg_dict[\"data\"][\"revision\"]][-1][:-8] + f\"_predictions_ft.parquet\",  filesystem=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = df_res[\"APE_NIV5\"]\n",
    "torchft_preds = df_res[\"APE_NIV5_pred_k1\"]\n",
    "fasttext_preds_labels = df_res_ft[\"APE_NIV5_pred_k1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((torchft_preds == fasttext_preds_labels).mean())\n",
    "print((torchft_preds == ground_truth).mean())\n",
    "print((ground_truth == fasttext_preds_labels).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some results\n",
    "sorted_confidence, well_predicted, predicted_confidence, predicted_class, true_values = (\n",
    "    sort_and_get_pred(predictions=torch_preds, df=df_test, Y=Y)\n",
    ")\n",
    "fig1 = confidence_histogram(sorted_confidence, well_predicted, df=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "well_predicted = (ground_truth == fasttext_preds_labels)\n",
    "print(well_predicted.shape)\n",
    "df = pd.DataFrame(\n",
    "        {\n",
    "            \"confidence_score\": fasttext_preds_scores.reshape((-1, )),\n",
    "            \"well_predicted\": well_predicted,  # Ensure this is categorical if needed\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Plot with proper data format\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.histplot(data=df, x=\"confidence_score\", bins=100, hue=\"well_predicted\", stat=\"percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy on automatically coded samples vs rate of automatic coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 100)\n",
    "torchft_scores = sorted_confidence[:, 0] - sorted_confidence[:, 1:5].sum(axis = 1)\n",
    "torchft_plot =  get_automatic_accuracy(thresholds, torch.clamp(torchft_scores, 0, 1).numpy(), predicted_class.numpy(), true_values)\n",
    "ft_plot =  get_automatic_accuracy(thresholds, np.clip(fasttext_preds_scores.reshape(-1), 0, 1), fasttext_preds_labels.reshape(-1), ground_truth.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file.py\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming thresholds, torchft_plot, fasttext_preds_scores, fasttext_preds_labels, and ground_truth are already defined\n",
    "\n",
    "# Create masks for the plots\n",
    "mask_torchft = torchft_plot[0] > 0\n",
    "mask_ft = ft_plot[0] > 0\n",
    "\n",
    "# Create the Plotly figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for torchft\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=torchft_plot[0][mask_torchft],\n",
    "    y=torchft_plot[1][mask_torchft],\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    text=[f'Threshold: {thresh}' for thresh in thresholds[mask_torchft]],\n",
    "    name='torchft'\n",
    "))\n",
    "\n",
    "# Add traces for ft\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=ft_plot[0][mask_ft],\n",
    "    y=ft_plot[1][mask_ft],\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    text=[f'({round(ft_plot[0][mask_ft], 3)},{round(ft_plot[1][mask_ft], 3)}), Threshold: {thresh}' for thresh in thresholds[mask_ft]],\n",
    "    name='ft'\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Pourcentage de codif automatique\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    legend=dict(\n",
    "        x=1,\n",
    "        y=1,\n",
    "        traceorder=\"normal\",\n",
    "        font=dict(\n",
    "            family=\"sans-serif\",\n",
    "            size=12,\n",
    "            color=\"black\"\n",
    "        ),\n",
    "        bgcolor=\"LightSteelBlue\",\n",
    "        bordercolor=\"Black\",\n",
    "        borderwidth=2\n",
    "    ),\n",
    "    width=800,  # Set the figure width\n",
    "    height=600   # Set the figure height\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disagreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_mask = (torchft_preds != ground_truth) & (fasttext_preds_labels == ground_truth)\n",
    "\n",
    "ground_truth_disagreements = ground_truth[filter_mask]\n",
    "torchft_disagreements = torchft_preds[filter_mask]\n",
    "fasttext_disagreements = pd.Series(fasttext_preds_labels)[filter_mask]\n",
    "\n",
    "disagreements = pd.DataFrame({\"ground_truth\": ground_truth_disagreements, \"torchft\": torchft_disagreements, \"fasttext\": fasttext_disagreements})\n",
    "print(disagreements.value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(check_df, filename = \"output.txt\", mode = 'a'):\n",
    "        with open(filename, mode) as file:\n",
    "            for index, row in check_df.iterrows():\n",
    "                if index==0:\n",
    "                    torchft_flag = \"PREDICTION CORRECTE\" if row['APE_NIV5'] == row['APE_NIV5_pred_k1_x'] else \"PREDICTION INCORRECTE\"\n",
    "                    ft_flag = \"PREDICTION CORRECTE\" if row['APE_NIV5'] == row['APE_NIV5_pred_k1_y'] else \"PREDICTION INCORRECTE\"\n",
    "\n",
    "                    file.write(f\"APE_NIV5 Code: {row['APE_NIV5']}\\n\")\n",
    "                    file.write(f\"LIB_NIV5: {row['LIB_NIV5']}\\n\")\n",
    "                    file.write(f\"TorchFastText Prediction - Code: {row['APE_NIV5_pred_k1_x']}, Label: {row['LIB_NIV5_pred_k1_x']}- {torchft_flag}\\n\")\n",
    "                    file.write(f\"FastText Prediction - Code: {row['APE_NIV5_pred_k1_y']}, Label: {row['LIB_NIV5_pred_k1_y']} - {ft_flag}\\n\")\n",
    "                    \n",
    "                    file.write(\"-\" * 50 + \"\\n\")\n",
    "                    file.write(\"Exemple de libell√©s :\\n\")\n",
    "                file.write(f\"{row['libelle']}\\n\")\n",
    "                file.write(f\"- TYP: {row['TYP']}, NAT: {row['NAT']}, SRF: {row['SRF']}, CJ: {row['CJ']}, CRT: {row['CRT']}\\n\")\n",
    "            file.write(\"\\n\")\n",
    "            file.write(\"=\"*75 + \"\\n\")\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INV_MAPPINGS = {}\n",
    "\n",
    "for key in list(mappings.keys()):\n",
    "    INV_MAPPINGS[key] = {v: k for k, v in mappings[key].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_filter = [\"libelle\" ,  \"APE_NIV5\", \"LIB_NIV5\", \"APE_NIV5_pred_k1\", \"LIB_NIV5_pred_k1\", \"proba_k1\"]\n",
    "cat_var =  [\"TYP\",\"NAT\", \"SRF\",\t\"CJ\", \"CRT\"]\n",
    "filename=\"torchIncorrect_ftCorrect.txt\"\n",
    "\n",
    "\n",
    "df_torchft = df_res[columns_filter + cat_var]\n",
    "df_ft = df_res_ft[columns_filter]\n",
    "\n",
    "generate_text(pd.DataFrame(), filename=filename, mode='w')\n",
    "for row in disagreements.value_counts().head(10).items():\n",
    "    gt, torchft, ft = row[0]\n",
    "    torch_ft = df_torchft[(df_torchft[\"APE_NIV5\"] == gt) & (df_torchft[\"APE_NIV5_pred_k1\"] == torchft)]\n",
    "    ft = df_ft[(df_ft[\"APE_NIV5\"] == gt) & (df_ft[\"APE_NIV5_pred_k1\"] == ft)]\n",
    "    check = torch_ft.merge(ft, on=[\"libelle\", \"APE_NIV5\", \"LIB_NIV5\"], how=\"inner\").sample(5).reset_index()\n",
    "\n",
    "    for key in cat_var:\n",
    "        if key != \"SRF\":\n",
    "            check[key] = check[key].map(INV_MAPPINGS[key].get)\n",
    "    generate_text(check, filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-level accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Calculate accuracy for each level\n",
    "def calculate_accuracy(ground_truth_df, predictions_df, level):\n",
    "    ground_truth_col = f'APE_NIV{level}'\n",
    "    prediction_col = f'APE_NIV{level}_pred_k1'\n",
    "\n",
    "    correct_predictions = (ground_truth_df[ground_truth_col] == predictions_df[prediction_col]).sum()\n",
    "    total_predictions = len(ground_truth_df)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracy for levels 1 to 5\n",
    "accuracies_torch = {level: calculate_accuracy(df_res, df_res, level) for level in range(1, 6)}\n",
    "accuracies_ft = {level: calculate_accuracy(df_res, df_res_ft, level) for level in range(1, 6)}\n",
    "accuracies, accuracies_ft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the dictionaries\n",
    "df = pd.DataFrame({\n",
    "    'Model 1': accuracies,\n",
    "    'Model 2': accuracies_ft\n",
    "})\n",
    "\n",
    "# Rename the index to reflect levels\n",
    "df.index.name = 'Level'\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracies at different levels when errors at level 5 (concept of \"controlled error\")\n",
    "\n",
    "filter_mask = (torchft_preds != ground_truth)\n",
    "torch_errors = df_res[filter_mask]\n",
    "\n",
    "filter_mask = (fasttext_preds_labels != ground_truth)\n",
    "ft_errors = df_res_ft[filter_mask]\n",
    "\n",
    "accuracies_torch_errors = {level: calculate_accuracy(torch_errors, torch_errors, level) for level in range(1, 6)}\n",
    "accuracies_ft_errors = {level: calculate_accuracy(ft_errors, ft_errors, level) for level in range(1, 6)}\n",
    "\n",
    "accuracies_torch_errors, accuracies_ft_errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
