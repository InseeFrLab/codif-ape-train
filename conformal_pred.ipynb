{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"src/\"))\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import hydra\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "from hydra import initialize_config_dir\n",
    "from joblib import Memory\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "from evaluators import Evaluator\n",
    "from models import FastTextWrapper\n",
    "from src.datasets import SoftClassifDataset, TextClassificationDataModule\n",
    "from utils.data import PATHS, get_df_naf, get_file_system, get_processed_data, get_test_data, get_Y\n",
    "from utils.evaluation import (\n",
    "    get_fasttext_preds,\n",
    "    get_ground_truth,\n",
    "    get_label_mapping,\n",
    "    sort_and_get_pred,\n",
    ")\n",
    "from utils.mappings import mappings\n",
    "from utils.mlflow import create_or_restore_experiment\n",
    "from utils.validation_viz import (\n",
    "    calibration_curve,\n",
    "    confidence_histogram,\n",
    "    get_automatic_accuracy,\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_URI'] = \"https://projet-ape-mlflow.user.lab.sspcloud.fr/\" \n",
    "# model_name = \"FastText-pytorch\"\n",
    "# module = mlflow.pytorch.load_model(f\"models:/{model_name}/latest\")\n",
    "\n",
    "run_id = \"01fd012d1d8f45828a889efd8cb926ec\"\n",
    "logged_model = f'runs:/{run_id}/model'\n",
    "# Load model as a PyFuncModel.\n",
    "module = mlflow.pytorch.load_model(logged_model, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the artifact directory (e.g., to a temp dir)\n",
    "local_artifacts_path = mlflow.artifacts.download_artifacts(run_id=run_id)\n",
    "\n",
    "# Load the YAML config\n",
    "with open(f\"{local_artifacts_path}/hydra_config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg: DictConfig = OmegaConf.create(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = hydra.utils.instantiate(cfg.model.trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = TextClassificationDataModule(\n",
    "                    cfg.data,\n",
    "                    cfg.tokenizer,\n",
    "                    cfg.dataset,\n",
    "                    batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = datamodule.val_dataloader()\n",
    "batch = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(module, val_loader)\n",
    "predictions_tensor = torch.cat(predictions).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datamodule.df_val\n",
    "Y = datamodule.Y\n",
    "df_naf = get_df_naf(revision=cfg.data.revision)\n",
    "true_values = get_ground_truth(df, Y)\n",
    "sorted_confidence, predicted_confidence, predicted_class = sort_and_get_pred(predictions=predictions_tensor, true_values=true_values)\n",
    "naf_predictions = get_pred_nafs(predicted_class, revision=cfg.data.revision)\n",
    "all_level_preds = get_all_levels(naf_predictions, df_naf)\n",
    "all_level_ground_truth = get_all_levels(df, df_naf, col=Y, revision=cfg.data.revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_level_preds == all_level_ground_truth).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naf_predictions.merge(df_naf, left_on=\"APE_NIV5_pred\", right_on=\"APE_NIV5\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_preds_labels, fasttext_preds_scores = get_fasttext_preds(revision=cfg.data.revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = get_label_mapping(cfg.data.revision)\n",
    "fasttext_preds_labels_int = fasttext_preds_labels.map(mapping.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = get_ground_truth(datamodule.df_test, datamodule.Y)\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "ft_plot = get_automatic_accuracy(\n",
    "    thresholds,\n",
    "    np.clip(fasttext_preds_scores.values.reshape(-1), 0, 1),\n",
    "    fasttext_preds_labels_int.values.reshape(-1),\n",
    "    true_values,\n",
    ")\n",
    "mask_ft = ft_plot[0] > 0\n",
    "\n",
    "# Create the matplotlib figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot ft data\n",
    "ax.scatter(ft_plot[0][mask_ft], ft_plot[1][mask_ft], label=\"ft\")\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel(\"Pourcentage de codif automatique\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc=\"upper right\", fancybox=True, shadow=True)\n",
    "\n",
    "# Set grid\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.max(dim=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = datamodule.train_dataloader()\n",
    "batch = next(iter(train_loader))\n",
    "inputs, targets = batch[:-1], batch[-1]\n",
    "outputs = module.forward(inputs)\n",
    "loss = module.loss(outputs, targets)\n",
    "accuracy = module.accuracy_fn(outputs, targets.argmax(dim=1))\n",
    "loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "inputs, targets = batch[:-1], batch[-1]\n",
    "targets = targets / targets.sum(dim=1, keepdim=True)\n",
    "targets[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, categorical_variables = (\n",
    "            df_test[text_feature].values,\n",
    "            df_test[categorical_features].values,\n",
    "        )\n",
    "\n",
    "dataset = SoftClassifDataset(\n",
    "    texts=text,\n",
    "    categorical_variables=categorical_variables,\n",
    "    tokenizer=module.model.tokenizer,\n",
    "    outputs=df_test[Y].values,\n",
    "    similarity_coefficients=[0.01, 0.1, 0.1, 0.1, 0.5],\n",
    "    revision=cfg_dict[\"data\"][\"revision\"],\n",
    ")\n",
    "dataloader = dataset.create_dataloader(\n",
    "    batch_size=10, shuffle=False, num_workers=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = df_train.sample(frac=0.001)\n",
    "df_val = df_val.sample(frac=0.01)\n",
    "df_test = df_test.sample(frac=0.01)\n",
    "\n",
    "train_text, train_categorical_variables = (\n",
    "            df_train[cfg_dict[\"data\"][\"text_feature\"]].values,\n",
    "            df_train[cfg_dict[\"data\"][\"categorical_features\"]].values,\n",
    "        )\n",
    "val_text, val_categorical_variables = (\n",
    "    df_val[cfg_dict[\"data\"][\"text_feature\"]].values,\n",
    "    df_val[cfg_dict[\"data\"][\"categorical_features\"]].values,\n",
    ")\n",
    "test_text, test_categorical_variables = (\n",
    "    df_test[cfg_dict[\"data\"][\"text_feature\"]].values,\n",
    "    df_test[cfg_dict[\"data\"][\"categorical_features\"]].values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = TOKENIZERS[cfg_dict[\"tokenizer\"][\"tokenizer_name\"]](\n",
    "            **cfg_dict[\"tokenizer\"], training_text=train_text\n",
    "        )\n",
    "\n",
    "num_rows = tokenizer.num_tokens + tokenizer.get_nwords() + 1\n",
    "padding_idx = num_rows - 1\n",
    "num_classes = max(mappings[Y].values()) + 1\n",
    "categorical_vocab_sizes = []\n",
    "for feature in cfg_dict[\"data\"][\"categorical_features\"]:\n",
    "    if feature == \"SRF\":\n",
    "        categorical_vocab_sizes.append(5)\n",
    "    else:\n",
    "        categorical_vocab_sizes.append(max(mappings[feature].values()) + 1)\n",
    "\n",
    "model = MODELS[cfg_dict[\"model\"][\"model_name\"]](\n",
    "    **cfg_dict[\"model\"][\"model_params\"],\n",
    "    tokenizer=tokenizer,\n",
    "    num_rows=num_rows,\n",
    "    num_classes=num_classes,\n",
    "    categorical_vocabulary_sizes=categorical_vocab_sizes,\n",
    "    padding_idx=padding_idx,\n",
    ")\n",
    "\n",
    "loss = LOSSES[cfg_dict[\"model\"][\"train_params\"][\"loss_name\"]]()\n",
    "optimizer = OPTIMIZERS[\n",
    "    cfg_dict[\"model\"][\"train_params\"][\"optimizer_name\"]\n",
    "]  # without the () !\n",
    "scheduler = SCHEDULERS[cfg_dict[\"model\"][\"train_params\"][\"scheduler_name\"]]\n",
    "\n",
    "module = MODULES[cfg_dict[\"model\"][\"model_name\"]](\n",
    "    model=model,\n",
    "    loss=loss,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    **cfg_dict[\"model\"][\"train_params\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TRAINERS[cfg_dict[\"model\"][\"train_params\"][\"trainer_name\"]](\n",
    "            **cfg_dict[\"model\"][\"train_params\"],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_class = SoftClassifDataset\n",
    "\n",
    "\n",
    "train_dataset = dataset_class(\n",
    "    texts=train_text,\n",
    "    categorical_variables=train_categorical_variables,\n",
    "    tokenizer=tokenizer,\n",
    "    outputs=df_train[Y].values,\n",
    "    revision=cfg_dict[\"data\"][\"revision\"],\n",
    "    similarity_coefficients=similarity_coefficients,\n",
    ")\n",
    "val_dataset = dataset_class(\n",
    "    texts=val_text,\n",
    "    categorical_variables=val_categorical_variables,\n",
    "    tokenizer=tokenizer,\n",
    "    outputs=df_val[Y].values,\n",
    "    revision=cfg_dict[\"data\"][\"revision\"],\n",
    "    similarity_coefficients=similarity_coefficients,\n",
    ")\n",
    "\n",
    "test_dataset = dataset_class(\n",
    "    texts=test_text,\n",
    "    categorical_variables=test_categorical_variables,\n",
    "    tokenizer=tokenizer,\n",
    "    outputs=df_test[Y].values,\n",
    "    revision=cfg_dict[\"data\"][\"revision\"],\n",
    "    similarity_coefficients=similarity_coefficients,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = train_dataset.create_dataloader(\n",
    "    **cfg_dict[\"model\"][\"train_params\"]\n",
    ")\n",
    "val_dataloader = val_dataset.create_dataloader(**cfg_dict[\"model\"][\"train_params\"])\n",
    "test_dataloader = test_dataset.create_dataloader(\n",
    "    **cfg_dict[\"model\"][\"train_params\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(df, dataloader, suffix='val'):\n",
    "    \"\"\"\n",
    "    Run evaluation on the given dataloader and log the results.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = trainer.predict(module, dataloader) # accumulates predictions over batches\n",
    "    predictions_tensor = torch.cat(predictions).cpu().numpy() # (num_test_samples, num_classes)\n",
    "\n",
    "    # Use your aggregation function\n",
    "    aggregated_results = Evaluator.get_aggregated_preds(\n",
    "        df=df,\n",
    "        Y=Y,\n",
    "        predictions=predictions_tensor,\n",
    "        top_k=1\n",
    "    )\n",
    "\n",
    "    display(aggregated_results)\n",
    "\n",
    "    accuracy = Evaluator.compute_accuracies(aggregated_preds=aggregated_results, suffix=suffix)\n",
    "\n",
    "    return aggregated_results\n",
    "\n",
    "run_eval(df_val, val_dataloader, suffix='val')\n",
    "run_eval(df_test, test_dataloader, suffix='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
